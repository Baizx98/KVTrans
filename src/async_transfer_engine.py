import queue
import threading
import torch
from typing import List
from block_manager import Block, BlockManager
from cache_engine import CacheEngine
from event_monitor import EventMonitor
from utils import is_pin_memory_available


class AsyncTransferEngine:
    def __init__(
        self,
        block_manager: BlockManager,
        cache_engine: CacheEngine,
        transfer_unit: int,
        src_device: torch.device,
        dst_device: torch.device,
        name: str,
    ) -> None:
        self.block_manager = block_manager
        self.cache_engine = cache_engine
        self.transfer_unit = transfer_unit
        self.name = name

        self.src_device = src_device
        self.dst_device = dst_device

        self._condition = threading.Condition()
        self._shutdown = False

        self.event_monitor = EventMonitor()
        # TODO monitor queue çš„ä½¿ç”¨æ–¹æ³•åº”è¯¥é‡æ–°è€ƒè™‘ï¼Œåˆ°åº•æ˜¯ç”¨æ³¨å†Œè¿˜æ˜¯å‡½æ•°ä¼ é€’å‘¢

        self.transfer_stream = torch.cuda.Stream()

        self.transfer_thread = threading.Thread(target=self._run, name=f"{name}_thread")

    def start(self):
        """Start the transfer and monitor threads."""
        self.transfer_thread.start()

    def shutdown(self):
        with self._condition:
            self._shutdown = True
            self._condition.notify()
        if hasattr(self, "transfer_thread") and self.transfer_thread:
            self.transfer_thread.join()
        self.event_monitor.unregister()
        print(f"ğŸ”š {getattr(self, 'name', 'AsyncTransferEngine')} shutdown complete.")

    def update_transfer_unit(self, num_blocks: int, current_step: int) -> int:
        if is_pin_memory_available():
            self.transfer_unit = min(
                self.block_manager.num_gpu_blocks, self.transfer_unit * 2
            )
        else:
            self.transfer_unit = max(1, self.transfer_unit // 2)
        self.transfer_unit = min(self.transfer_unit, num_blocks - current_step)
        return self.transfer_unit

    def notify(self, layer: int):
        raise NotImplementedError(
            "Subclasses must implement the notify method to handle layer notifications."
        )

    # è‡ªå®šä¹‰çš„åˆ¤æ–­æ¡ä»¶
    def _should_wait(self) -> bool:
        return not self._shutdown

    def _get_task(self):
        raise NotImplementedError

    def _transfer(self, task):
        raise NotImplementedError

    def _run(self):
        while True:
            with self._condition:
                while self._should_wait():
                    self._condition.wait()
                if self._shutdown:
                    break
                task = self._get_task()

            if task is not None:
                self._transfer(task)

    def _transfer_unit(self, plan: List[Block], blocks: List[Block]):
        """
        Transfer a unit of blocks according to the offload plan.
        This method should be implemented in subclasses.
        """
        blocks_to_transfer = torch.tensor(plan, device="cpu", dtype=torch.int64).view(
            -1, 2
        )

        def on_transfer_complete():
            # TODO è¯¥å‡½æ•°å°šæœªå®Œæˆï¼Œè¿˜ä¸èƒ½ä½¿ç”¨transfer unitå‡½æ•°
            self.block_manager.update_blocks_after_transfer()

        self.cache_engine.transfer_blocks_async(
            blocks_to_transfer,
            self.src_device,
            self.dst_device,
            self.transfer_stream,  # type: ignore
            callback_fn=on_transfer_complete,
            add_event=self.event_monitor.add_event,
        )
