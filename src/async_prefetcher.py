import queue
import threading
import torch
import time
from collections import deque
from queue import Queue
from block_manager import BlockManager, Block
from cache_engine import CacheEngine
from typing import List, Optional, Set, Callable, Tuple
from utils import is_pin_memory_available


class AsyncPrefetcher:
    def __init__(
        self, block_manager: BlockManager, cache_engine: CacheEngine, transfer_unit: int
    ):
        """
        Args:
            block_manager: å—ç®¡ç†å™¨
            cache_engine: ç¼“å­˜å¼•æ“
            transfer_unit: ä¼ è¾“å•ä½
        """
        self.block_manager = block_manager
        self.cache_engine = cache_engine
        self.transfer_unit = transfer_unit
        self.watermark = self.block_manager.watermark

        self.prefetch_queue: deque[int] = deque()
        self._condition = threading.Condition()
        self._shutdown = False
        self._monitor_shutdown = False
        self._prefetched_layers: Set[int] = set()

        self.prefetch_thread: Optional[threading.Thread] = threading.Thread(
            target=self._run, daemon=True, name="prefetch_thread"
        )
        self.prefetch_thread.start()

        self.event_monitor_thread: Optional[threading.Thread] = threading.Thread(
            target=self._event_monitor_worker,
            daemon=True,
            name="prefetch_event_monitor_thread",
        )
        self.event_monitor_thread.start()

        # å½“engineä¸­çš„generateä¸­è®¡ç®—å®Œä¸€ä¸ªå±‚åï¼Œéœ€è¦æŠŠè¯¥å±‚çš„å±‚å·å‘ç»™prefetcher
        # prefetcheréœ€è¦ä»è¯¥å±‚çš„ä¸‹ä¸€å±‚å¼€å§‹ï¼Œæ£€æŸ¥è¯¥å±‚ä»¥åæœ€è¿‘çš„å±‚ä¸­çš„åœ¨CPUä¸­çš„å—
        # æŠŠè¿™äº›å—æŒ‰ç…§å•ä½unitè¿›è¡Œæµå¼æ‹·è´åˆ°GPUä¸Šï¼Œç›´åˆ°è¯¥å±‚æ‰€éœ€è¦çš„æœ€å°‘å¿«éƒ½åœ¨GPUä¸Š
        # è¿™æ—¶ï¼Œè¯¥å±‚å¼€å§‹è®¡ç®—ï¼Œå¹¶ç»ˆæ­¢é¢„å–è¿›ç¨‹é¢„å–è¯¥å±‚çš„å—

    def _get_prefetch_layers(self, current_layer: int) -> List[int]:
        """
        å¯æ ¹æ®ç³»ç»Ÿè´Ÿè½½åŠ¨æ€è°ƒæ•´é¢„å–å±‚æ•°ã€‚
        å½“å‰å®ç°ä¸ºé™æ€ï¼šé¢„å– current_layer + 1 å’Œ +2ã€‚
        """
        # TODO è¿™é‡Œéœ€è¦æ ¹æ®watermarkæ¥å†³å®šé¢„å–çš„å±‚æ•°
        # TODO è¿™é‡Œé¢„å–çš„å±‚æ•°æ˜¯å¾ªç¯çš„ï¼Œé¢„å–å®Œæœ€åä¸€å±‚åå°±åº”è¯¥é¢„å–ç¬¬ä¸€å±‚äº†
        return [
            l
            for l in range(current_layer + 1, current_layer + 3)
            if l < self.block_manager.num_attn_layers
            and l not in self._prefetched_layers
        ]

    def notify(self, layer: int):
        with self._condition:
            # TODO è¿™é‡Œéœ€è¦æ ¹æ®watermarkæ¥å†³å®šé¢„å–çš„å±‚æ•°
            for future_layer in self._get_prefetch_layers(layer):
                self.prefetch_queue.append(future_layer)
                self._prefetched_layers.add(future_layer)
            self._condition.notify()

    def shutdown(self):
        with self._condition:
            self._shutdown = True
            self._condition.notify()
        self._monitor_shutdown = True
        if self.event_monitor_thread:
            self.event_monitor_thread.join()
        if self.prefetch_thread:
            self.prefetch_thread.join()
        print("ğŸŸ¢ AsyncPrefetcher shutdown complete.")

    def update_transfer_unit(self, num_blocks: int, current_step: int) -> int:
        """æ ¹æ®ä¼ è¾“å¸¦å®½åˆ©ç”¨ç‡ï¼Œæ›´æ–°ä¼ è¾“å•ä½ï¼Œå¦‚æœPCIeå¸¦å®½åˆ©ç”¨ç‡ä½ï¼Œåˆ™å¢åŠ ä¼ è¾“å•ä½ï¼Œå¦åˆ™å‡å°ä¼ è¾“å•ä½"""
        # FIXME: éœ€è¦æ ¹æ®ä¼ è¾“å¸¦å®½åˆ©ç”¨ç‡æ¥æ›´æ–°ä¼ è¾“å•ä½
        if is_pin_memory_available():
            self.transfer_unit = min(
                self.block_manager.num_gpu_blocks, self.transfer_unit * 2
            )
        else:
            self.transfer_unit = max(1, self.transfer_unit // 2)
        self.transfer_unit = min(self.transfer_unit, num_blocks - current_step)
        return self.transfer_unit

    def _run(self):
        while True:
            with self._condition:
                while not self.prefetch_queue and not self._shutdown:
                    self._condition.wait()
                if self._shutdown:
                    break
                layer = self.prefetch_queue.popleft()
                self._prefetched_layers.remove(layer)
            print(f"ğŸŸ¢ Layer {layer} prefetch started.")
            sorted_blocks = self.block_manager.predict_next_layer_needed_blocks(layer)
            num_blocks = len(sorted_blocks)
            current_step = 0

            if num_blocks == 0:
                continue

            # âœ³ï¸ å¦‚æœ GPU å—æ•°é‡è§¦åŠæ°´ä½çº¿ï¼Œåˆ™å¯ä»¥è§¦å‘ offloadï¼ˆä½ å¯ä»¥è‡ªå®šä¹‰è°ƒç”¨ï¼‰
            if self.block_manager.gpu_free_block_num() < int(
                self.block_manager.num_gpu_blocks * self.watermark
            ):
                print(f"âš ï¸ Layer {layer} é¢„å–å‰è§¦åŠ GPU æ°´ä½çº¿ï¼Œå»ºè®®è§¦å‘å¸è½½ç­–ç•¥")

            while current_step < num_blocks:
                blocks = sorted_blocks[current_step : current_step + self.transfer_unit]
                if not blocks:
                    break
                try:
                    plan = self.block_manager.get_prefetch_plan(blocks)
                    print(
                        f"prefetch plan for layer {layer} at step {current_step}: {plan}"
                    )
                except RuntimeError as e:
                    print(f"ğŸŸ¥ Layer {layer} prefetch failed: {e}")
                    break

                self._prefetch_unit(plan, blocks)
                current_step += self.transfer_unit

            print(f"âœ… Layer {layer} é¢„å–å®Œæˆ")

    def _prefetch_unit(self, plan: List[tuple[int, int]], blocks: List[Block]):
        """ç”Ÿæˆé¢„å–è®¡åˆ’çš„tensor,å¹¶æ‰§è¡Œå¼‚æ­¥æ‹·è´ï¼Œæ‹·è´å®Œæˆåæ›´æ–°blockçš„device"""
        blocks_to_prefetch = torch.tensor(plan, device="cpu", dtype=torch.int64).view(
            -1, 2
        )

        def on_transfer_complete():
            self.block_manager.update_block_device_prefetch(plan, blocks)

        self.cache_engine.prefetch_copy_blocks_async(
            blocks_to_prefetch,
            blocks,
            callback_fn=on_transfer_complete,
        )

    def _event_monitor_worker(self):
        """è¿™ä¸ªçº¿ç¨‹ä¸“é—¨è´Ÿè´£æ£€æŸ¥CUDAäº‹ä»¶æ˜¯å¦å®Œæˆï¼Œå¹¶åœ¨å®Œæˆåæ‰§è¡Œå›è°ƒ"""
        # TODO æ·»åŠ å®šæ—¶å›è°ƒå¤„ç†æˆ–æ‰¹é‡å›è°ƒå¤„ç†æœºåˆ¶ï¼Œå‡å°‘è°ƒåº¦å’Œè½®è¯¢å¼€é”€
        print("ğŸ’¡ Prefetch event monitor thread started.")
        pending_events: List[Tuple[torch.cuda.Event, Callable]] = []
        BATCH_SIZE = 16
        WAIT_TIME = 0.001
        while not self._monitor_shutdown:
            # print("ğŸŸ¢ prefetch callback running")
            # TODO æ£€æŸ¥æ˜¯å¦æœ‰ä»»åŠ¡éœ€è¦ä¸­æ­¢
            try:
                for _ in range(BATCH_SIZE - len(pending_events)):
                    event, callback_fn = (
                        self.cache_engine.prefetch_monitor_queue.get_nowait()
                    )
                    pending_events.append((event, callback_fn))
            except queue.Empty:
                pass

            if not pending_events:
                time.sleep(WAIT_TIME)
                continue

            for event, callback_fn in pending_events:
                while not event.query():
                    time.sleep(WAIT_TIME)

            ready_indices = []
            for i, (event, _) in enumerate(pending_events):
                if event.query():
                    ready_indices.append(i)

            for idx in reversed(ready_indices):
                _, callback_fn = pending_events.pop(idx)
                callback_fn()

            time.sleep(WAIT_TIME)
