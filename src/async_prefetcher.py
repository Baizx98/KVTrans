import threading
import torch
import time
from block_manager import BlockManager, Block
from cache_engine import CacheEngine
from typing import List, Optional
from utils import is_pin_memory_available

class AsyncPrefetcher:
    def __init__(self, block_manager, cache_engine, transfer_unit):
        self.block_manager = block_manager
        self.cache_engine = cache_engine
        self.transfer_unit = transfer_unit

        self.prefetch_thread:Optional[threading.Thread] = None
        self.abort_event = threading.Event()
        self.lock = threading.Lock()

        # å½“engineä¸­çš„generateä¸­è®¡ç®—å®Œä¸€ä¸ªå±‚åï¼Œéœ€è¦æŠŠè¯¥å±‚çš„å±‚å·å‘ç»™prefetcher
        # prefetcheréœ€è¦ä»è¯¥å±‚çš„ä¸‹ä¸€å±‚å¼€å§‹ï¼Œæ£€æŸ¥è¯¥å±‚ä»¥åæœ€è¿‘çš„å±‚ä¸­çš„åœ¨CPUä¸­çš„å—  
        # æŠŠè¿™äº›å—æŒ‰ç…§å•ä½unitè¿›è¡Œæµå¼æ‹·è´åˆ°GPUä¸Šï¼Œç›´åˆ°è¯¥å±‚æ‰€éœ€è¦çš„æœ€å°‘å¿«éƒ½åœ¨GPUä¸Š
        # è¿™æ—¶ï¼Œè¯¥å±‚å¼€å§‹è®¡ç®—ï¼Œå¹¶ç»ˆæ­¢é¢„å–è¿›ç¨‹é¢„å–è¯¥å±‚çš„å—

    def update_transfer_unit(self, num_blocks: int, current_step: int) -> int:
        """æ ¹æ®ä¼ è¾“å¸¦å®½åˆ©ç”¨ç‡ï¼Œæ›´æ–°ä¼ è¾“å•ä½ï¼Œå¦‚æœPCIeå¸¦å®½åˆ©ç”¨ç‡ä½ï¼Œåˆ™å¢åŠ ä¼ è¾“å•ä½ï¼Œå¦åˆ™å‡å°ä¼ è¾“å•ä½"""
        # FIXME: éœ€è¦æ ¹æ®ä¼ è¾“å¸¦å®½åˆ©ç”¨ç‡æ¥æ›´æ–°ä¼ è¾“å•ä½
        if is_pin_memory_available():
            self.transfer_unit = min(
                self.block_manager.num_gpu_blocks, self.transfer_unit * 2
            )
        else:
            self.transfer_unit = max(1, self.transfer_unit // 2)
        self.transfer_unit = min(self.transfer_unit, num_blocks - current_step)
        return self.transfer_unit

    def start_prefetch(self,layer:int):
        """åœ¨é¢„å–layerå±‚çš„kvæ—¶,åœ¨ç¬¬layerå±‚è¿›è¡Œè®¡ç®—ä¹‹å‰,å¿…é¡»æŠŠè¯¥å±‚æ‰€éœ€è¦çš„æœ€å°‘KVå—æ‹·è´åˆ°GPUä¸Š"""
        # å…¶å®ä¹Ÿæ˜¯
        with self.lock:
            if self.prefetch_thread and self.prefetch_thread.is_alive():
                self.abort_event.set()
                self.prefetch_thread.join()
                print("ğŸŸ¥ Previous prefetch task aborted.")
            self.abort_event.clear()
            self.prefetch_thread = threading.Thread(target=self._prefetch_worker, args=(layer,))
            self.prefetch_thread.start()

    def _prefetch_worker(self,layer:int):
        sorted_blocks = self.block_manager.predict_next_layer_needed_blocks(layer)
        num_blocks = len(sorted_blocks)
        current_step = 0

        while current_step < num_blocks:
            if self.abort_event.is_set():
                print(f"ğŸŸ¡ Layer {layer} prefetch interrupted at step {current_step}.")
                break
            # self.transfer_unit = self.update_transfer_unit(num_blocks, current_step)
            print(f"tranfer unit is {self.transfer_unit}")
            # FIXME æ­¤å¤„åˆ‡ç‰‡ç´¢å¼•æ˜¯å¦ä¼šè¶Šç•Œå‘¢
            blocks = sorted_blocks[current_step : current_step + self.transfer_unit]
            if not blocks:
                break
            plan = self.block_manager.get_prefetch_plan(blocks)
            print(f"prefetch plan for layer {layer} at step {current_step}: {plan}")
            self._prefetch_unit(plan, blocks)
            current_step += self.transfer_unit
        print(f"âœ… Layer {layer} prefetch complete.")
    
    def _prefetch_unit(self, plan: List[tuple[int, int]], blocks: List[Block]):
        """ç”Ÿæˆé¢„å–è®¡åˆ’çš„tensor,å¹¶æ‰§è¡Œå¼‚æ­¥æ‹·è´ï¼Œæ‹·è´å®Œæˆåæ›´æ–°blockçš„device"""
        blocks_to_prefetch = torch.tensor(plan, device="cpu", dtype=torch.int64).view(
            -1, 2
        )

        def on_transfer_complete():
            self.block_manager.update_block_device_prefetch(plan, blocks)

        self.cache_engine.prefetch_copy_blocks_async(
            blocks_to_prefetch,   
            blocks,
            callback_fn=on_transfer_complete,
        )
    
    def _event_monitor_worker(self):
        """è¿™ä¸ªçº¿ç¨‹ä¸“é—¨è´Ÿè´£æ£€æŸ¥CUDAäº‹ä»¶æ˜¯å¦å®Œæˆï¼Œå¹¶åœ¨å®Œæˆåæ‰§è¡Œå›è°ƒ"""
        print("ğŸ’¡ Prefetch event monitor thread started.")
        while True:
            # æ£€æŸ¥æ˜¯å¦æœ‰ä»»åŠ¡éœ€è¦ä¸­æ­¢
            if self.abort_event.is_set() and self.prefetch_thread and not self.prefetch_thread.is_alive():
                # FIXME prefetch_thread åœ¨åˆ‡æ¢æ—¶ä¹Ÿä¼šç»ˆæ­¢ï¼Œè¿™é‡Œæœ‰å¯èƒ½ä¼šæ„å¤–è§¦å‘æ”¹é€»è¾‘
                # å¦‚æœä¸» prefetch çº¿ç¨‹å·²ç»ä¸­æ­¢ä¸”ä¸æ´»è·ƒï¼Œå¯ä»¥è€ƒè™‘åœæ­¢ç›‘æ§çº¿ç¨‹
                # æˆ–è€…è®©å®ƒç»§ç»­ç­‰å¾…æ–°çš„ prefetch ä»»åŠ¡
                # ä¸ºç®€å•èµ·è§ï¼Œè¿™é‡Œè®©å®ƒä¸€ç›´è¿è¡Œ
                pass
            
            events_to_process = []
            with self.cache_engine.prefetch_callbacks_lock:
                # éå†æ‰€æœ‰å¾…å¤„ç†çš„äº‹ä»¶
                for event, callback_fn in list(
                    self.cache_engine.prefetch_completion_callbacks.items()
                ):
                    if event.query():  # æ£€æŸ¥äº‹ä»¶æ˜¯å¦å®Œæˆ
                        events_to_process.append((event, callback_fn))
            
            for event, callback_fn in events_to_process:
                # ç§»é™¤å·²å®Œæˆçš„äº‹ä»¶ è¿™é‡Œæ˜¯ä»€ä¹ˆæ„æ€å‘¢ï¼ŸNOTE
                with self.cache_engine.prefetch_callbacks_lock:
                    del self.cache_engine.prefetch_completion_callbacks[event]
                # æ‰§è¡Œå›è°ƒå‡½æ•°
                callback_fn()

            time.sleep(0.001)  # çŸ­æš‚ä¼‘çœ ï¼Œé¿å…å¿™ç­‰å¾…æ¶ˆè€—è¿‡å¤š CPU