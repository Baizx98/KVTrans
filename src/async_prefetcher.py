import queue
import torch
import time
from collections import deque
from block_manager import BlockManager, Block
from cache_engine import CacheEngine
from async_transfer_engine import AsyncTransferEngine
from typing import List, Set, Callable, Tuple


class AsyncPrefetcher(AsyncTransferEngine):
    def __init__(
        self, block_manager: BlockManager, cache_engine: CacheEngine, transfer_unit: int
    ):
        """
        Args:
            block_manager: å—ç®¡ç†å™¨
            cache_engine: ç¼“å­˜å¼•æ“
            transfer_unit: ä¼ è¾“å•ä½
        """
        super().__init__(
            block_manager,
            cache_engine,
            transfer_unit,
            src_device=torch.device("cpu"),
            dst_device=torch.device("cuda"),
            name="AsyncPrefetcher",
        )

        # ç‰¹æœ‰çš„å˜é‡
        self.watermark = self.block_manager.watermark
        self.prefetch_queue: deque[int] = deque()
        self._prefetched_layers: Set[int] = set()

        self.start()

        # å½“engineä¸­çš„generateä¸­è®¡ç®—å®Œä¸€ä¸ªå±‚åï¼Œéœ€è¦æŠŠè¯¥å±‚çš„å±‚å·å‘ç»™prefetcher
        # prefetcheréœ€è¦ä»è¯¥å±‚çš„ä¸‹ä¸€å±‚å¼€å§‹ï¼Œæ£€æŸ¥è¯¥å±‚ä»¥åæœ€è¿‘çš„å±‚ä¸­çš„åœ¨CPUä¸­çš„å—
        # æŠŠè¿™äº›å—æŒ‰ç…§å•ä½unitè¿›è¡Œæµå¼æ‹·è´åˆ°GPUä¸Šï¼Œç›´åˆ°è¯¥å±‚æ‰€éœ€è¦çš„æœ€å°‘å¿«éƒ½åœ¨GPUä¸Š
        # è¿™æ—¶ï¼Œè¯¥å±‚å¼€å§‹è®¡ç®—ï¼Œå¹¶ç»ˆæ­¢é¢„å–è¿›ç¨‹é¢„å–è¯¥å±‚çš„å—

    def _get_prefetch_layers(self, current_layer: int) -> List[int]:
        """
        å¯æ ¹æ®ç³»ç»Ÿè´Ÿè½½åŠ¨æ€è°ƒæ•´é¢„å–å±‚æ•°ã€‚
        å½“å‰å®ç°ä¸ºé™æ€ï¼šé¢„å– current_layer + 1 å’Œ +2ã€‚
        """
        # TODO è¿™é‡Œéœ€è¦æ ¹æ®watermarkæ¥å†³å®šé¢„å–çš„å±‚æ•°
        # TODO è¿™é‡Œé¢„å–çš„å±‚æ•°æ˜¯å¾ªç¯çš„ï¼Œé¢„å–å®Œæœ€åä¸€å±‚åå°±åº”è¯¥é¢„å–ç¬¬ä¸€å±‚äº†
        return [
            layer
            for layer in range(current_layer + 1, current_layer + 3)
            if layer < self.block_manager.num_attn_layers
            and layer not in self._prefetched_layers
        ]

    def notify(self, layer: int):
        with self._condition:
            # TODO è¿™é‡Œéœ€è¦æ ¹æ®watermarkæ¥å†³å®šé¢„å–çš„å±‚æ•°
            for future_layer in self._get_prefetch_layers(layer):
                self.prefetch_queue.append(future_layer)
                self._prefetched_layers.add(future_layer)
            self._condition.notify()

    def _should_wait(self) -> bool:
        return not self.prefetch_queue and not self._shutdown

    def _get_task(self):
        layer = self.prefetch_queue.popleft()
        self._prefetched_layers.remove(layer)
        return layer

    def _transfer(self, task):
        layer = task
        sorted_blocks = self.block_manager.predict_next_layer_needed_blocks(layer)
        num_blocks = len(sorted_blocks)
        current_step = 0

        if num_blocks == 0:
            return

        # âœ³ï¸ å¦‚æœ GPU å—æ•°é‡è§¦åŠæ°´ä½çº¿ï¼Œåˆ™å¯ä»¥è§¦å‘ offloadï¼ˆä½ å¯ä»¥è‡ªå®šä¹‰è°ƒç”¨ï¼‰
        if self.block_manager.gpu_free_block_num() < int(
            self.block_manager.num_gpu_blocks * self.watermark
        ):
            print(f"âš ï¸ Layer {layer} é¢„å–å‰è§¦åŠ GPU æ°´ä½çº¿ï¼Œå»ºè®®è§¦å‘å¸è½½ç­–ç•¥")

        while current_step < num_blocks:
            blocks = sorted_blocks[current_step : current_step + self.transfer_unit]
            if not blocks:
                break
            try:
                plan = self.block_manager.get_prefetch_plan(blocks)
                print(f"prefetch plan for layer {layer} at step {current_step}: {plan}")
            except RuntimeError as e:
                print(f"ğŸŸ¥ Layer {layer} prefetch failed: {e}")
                break

            self._prefetch_unit(plan, blocks)
            current_step += self.transfer_unit

        print(f"âœ… Layer {layer} é¢„å–å®Œæˆ")

    def _prefetch_unit(self, plan: List[tuple[int, int]], blocks: List[Block]):
        """ç”Ÿæˆé¢„å–è®¡åˆ’çš„tensor,å¹¶æ‰§è¡Œå¼‚æ­¥æ‹·è´ï¼Œæ‹·è´å®Œæˆåæ›´æ–°blockçš„device"""
        blocks_to_prefetch = torch.tensor(plan, device="cpu", dtype=torch.int64).view(
            -1, 2
        )

        def on_transfer_complete():
            self.block_manager.update_block_device_prefetch(plan, blocks)

        # self.cache_engine.prefetch_copy_blocks_async(
        #     blocks_to_prefetch,
        #     blocks,
        #     callback_fn=on_transfer_complete,
        # )
        self.cache_engine.transfer_blocks_async(
            blocks_to_prefetch,
            self.src_device,
            self.dst_device,
            self.transfer_stream,  # type: ignore
            callback_fn=on_transfer_complete,
        )

    def _event_monitor_worker(self):
        """è¿™ä¸ªçº¿ç¨‹ä¸“é—¨è´Ÿè´£æ£€æŸ¥CUDAäº‹ä»¶æ˜¯å¦å®Œæˆï¼Œå¹¶åœ¨å®Œæˆåæ‰§è¡Œå›è°ƒ"""
        # TODO æ·»åŠ å®šæ—¶å›è°ƒå¤„ç†æˆ–æ‰¹é‡å›è°ƒå¤„ç†æœºåˆ¶ï¼Œå‡å°‘è°ƒåº¦å’Œè½®è¯¢å¼€é”€
        print("ğŸ’¡ Prefetch event monitor thread started.")
        pending_events: List[Tuple[torch.cuda.Event, Callable]] = []
        BATCH_SIZE = 16
        WAIT_TIME = 0.001
        while not self._monitor_shutdown:
            # print("ğŸŸ¢ prefetch callback running")
            # TODO æ£€æŸ¥æ˜¯å¦æœ‰ä»»åŠ¡éœ€è¦ä¸­æ­¢
            try:
                for _ in range(BATCH_SIZE - len(pending_events)):
                    event, callback_fn = (
                        self.cache_engine.prefetch_monitor_queue.get_nowait()
                    )
                    pending_events.append((event, callback_fn))
            except queue.Empty:
                pass

            if not pending_events:
                time.sleep(WAIT_TIME)
                continue

            for event, callback_fn in pending_events:
                while not event.query():
                    time.sleep(WAIT_TIME)

            ready_indices = []
            for i, (event, _) in enumerate(pending_events):
                if event.query():
                    ready_indices.append(i)

            for idx in reversed(ready_indices):
                _, callback_fn = pending_events.pop(idx)
                callback_fn()

            time.sleep(WAIT_TIME)
