import threading
import torch
import time
from block_manager import BlockManager, Block
from cache_engine import CacheEngine
from typing import List, Optional
from utils import is_pin_memory_available


class AsyncOffloader:
    def __init__(
        self, block_manager: BlockManager, cache_engine: CacheEngine, transfer_unit: int
    ):
        self.block_manager = block_manager
        self.cache_engine = cache_engine
        self.transfer_unit: int = transfer_unit

        self.offload_thread: Optional[threading.Thread] = None
        self.abort_event = threading.Event()
        self.lock = threading.Lock()
        self.event_monitor_thread = threading.Thread(
            target=self._event_monitor_worker, daemon=True
        )
        self.event_monitor_thread.start()

    def update_transfer_unit(self, num_blocks: int, current_step: int) -> int:
        # å¦‚æœæ¯æ¬¡ä¼ è¾“çš„å¤„ç†å¼€é”€å¤ªå¤§ï¼Œåˆ™å¢åŠ ä¼ è¾“å•ä½ï¼Œå¦åˆ™å‡å°ä¼ è¾“å•ä½
        if is_pin_memory_available():
            self.transfer_unit = min(
                self.block_manager.num_gpu_blocks, self.transfer_unit * 2
            )
        else:
            self.transfer_unit = max(1, self.transfer_unit // 2)
        self.transfer_unit = min(self.transfer_unit, num_blocks - current_step)
        return self.transfer_unit

    def start_offload(self, layer: int):
        with self.lock:
            # ä¸­æ­¢å·²æœ‰ä»»åŠ¡
            if self.offload_thread and self.offload_thread.is_alive():
                self.abort_event.set()
                self.offload_thread.join()

            self.abort_event.clear()
            self.offload_thread = threading.Thread(
                target=self._offload_worker, args=(layer,)
            )
            self.offload_thread.start()

    def _offload_worker(self, layer: int):
        sorted_blocks = self.block_manager.get_layer_blocks_by_importance(layer)

        num_blocks = len(sorted_blocks)
        current_step = 0

        while current_step < num_blocks:
            if self.abort_event.is_set():
                print(f"ğŸŸ¡ Layer {layer} offload interrupted at step {current_step}.")
                break

            # è·å–å½“å‰æ­¥é•¿çš„å—
            # self.transfer_unit = self.update_transfer_unit(num_blocks, current_step)
            print(f"tranfer unit is {self.transfer_unit}")
            blocks = sorted_blocks[current_step : current_step + self.transfer_unit]
            if not blocks:
                break
            # è¿™é‡Œè¿”å›GPUå—å’ŒCPUå—çš„ç‰©ç†id
            plan = self.block_manager.get_offload_plan(blocks)
            print(f"offload plan for layer {layer} at step {current_step}: {plan}")
            self._offload_unit(plan, blocks)
            current_step += self.transfer_unit
        print(f"âœ… Layer {layer} offload complete.")

    def _offload_unit(self, plan: List[tuple[int, int]], blocks: List[Block]):
        blocks_to_offload = torch.tensor(plan, device="cpu", dtype=torch.int64).view(
            -1, 2
        )

        # é—­åŒ…ï¼Œç®€åŒ–å‡½æ•°å‚æ•°ä¼ é€’
        def on_transfer_complete():
            self.block_manager.update_block_device(plan, blocks)

        self.cache_engine.copy_blocks_async(
            blocks_to_offload,
            blocks,
            callback_fn=on_transfer_complete,
        )

    def _event_monitor_worker(self):
        """
        è¿™ä¸ªçº¿ç¨‹ä¸“é—¨è´Ÿè´£æ£€æŸ¥ CUDA äº‹ä»¶æ˜¯å¦å®Œæˆï¼Œå¹¶åœ¨å®Œæˆåæ‰§è¡Œå›è°ƒã€‚
        """
        print("ğŸ’¡ Event monitor thread started.")
        while True:
            # æ£€æŸ¥æ˜¯å¦æœ‰ä»»åŠ¡éœ€è¦ä¸­æ­¢
            if self.abort_event.is_set() and not self.offload_thread.is_alive():
                # å¦‚æœä¸» offload çº¿ç¨‹å·²ç»ä¸­æ­¢ä¸”ä¸æ´»è·ƒï¼Œå¯ä»¥è€ƒè™‘åœæ­¢ç›‘æ§çº¿ç¨‹
                # æˆ–è€…è®©å®ƒç»§ç»­ç­‰å¾…æ–°çš„ offload ä»»åŠ¡
                # ä¸ºç®€å•èµ·è§ï¼Œè¿™é‡Œè®©å®ƒä¸€ç›´è¿è¡Œ
                pass

            events_to_process = []
            with self.cache_engine.callbacks_lock:
                # éå†æ‰€æœ‰å¾…å¤„ç†çš„äº‹ä»¶
                for event, callback_fn in list(
                    self.cache_engine.completion_callbacks.items()
                ):
                    if event.query():  # æ£€æŸ¥äº‹ä»¶æ˜¯å¦å®Œæˆ
                        events_to_process.append((event, callback_fn))

            for event, callback_fn in events_to_process:
                # ç§»é™¤å·²å®Œæˆçš„äº‹ä»¶ è¿™é‡Œæ˜¯ä»€ä¹ˆæ„æ€å‘¢ï¼ŸNOTE
                with self.cache_engine.callbacks_lock:
                    del self.cache_engine.completion_callbacks[event]
                # æ‰§è¡Œå›è°ƒå‡½æ•°
                callback_fn()

            time.sleep(0.001)  # çŸ­æš‚ä¼‘çœ ï¼Œé¿å…å¿™ç­‰å¾…æ¶ˆè€—è¿‡å¤š CPU
