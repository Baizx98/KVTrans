"""This is engine with async kv streaming block transfer."""

from block_manager import BlockManager
from scheduler import cleanup_batch, schedule_batch
from worker import Worker
from async_offloader import AsyncOffloader
from async_prefetcher import AsyncPrefetcher
from config import CacheConfig, ModelConfig, DeviceConfig
from sequence import Sequence
from typing import List


class Engine:
    """Engine class for managing the KV Cache with async streaming block transfer."""

    def __init__(
        self,
        cache_config: CacheConfig,
        model_config: ModelConfig,
        device_config: DeviceConfig,
    ):
        """Initialize the Engine with cache, model, and device configurations."""
        self.cache_config = cache_config
        self.model_config = model_config
        self.device_config = device_config

        self.block_size = cache_config.block_size
        self.num_gpu_blocks = cache_config.num_gpu_blocks
        self.num_cpu_blocks = cache_config.num_cpu_blocks

        self.dtype = model_config.dtype
        self.device = device_config.device

        self.block_manager = BlockManager(cache_config, model_config, device_config)
        self.worker = Worker(cache_config, model_config, device_config)
        self.block_transfer_unit = cache_config.transfer_unit
        self.async_offloader = AsyncOffloader(
            self.block_manager,
            self.worker.cache_engine,
            self.block_transfer_unit,
        )
        self.async_prefetcher = AsyncPrefetcher(
            self.block_manager,
            self.worker.cache_engine,
            self.block_transfer_unit,
        )

        self.prefill_flag = True

    def generate(self, batch: List[Sequence]):
        while batch:
            # scheduled_batch å’Œ batch æ˜¯åŒä¸€ä¸ªå¯¹è±¡
            scheduled_batch = schedule_batch(batch)
            print(f"scheduled batch nums: {len(scheduled_batch)}")
            # æ¨ç†
            self.step(batch)
            # æ¨¡æ‹Ÿæ–°tokenç”Ÿæˆå’Œç”Ÿæˆç»“æŸ
            for sequence in batch:
                sequence.generate_new_token()
            # æ¸…ç† è¿™é‡Œçš„ä¸¤ä¸ªbatchä¸æ˜¯åŒä¸€ä¸ªå¯¹è±¡
            batch = cleanup_batch(batch)
        print("All sequences processed and cleaned up.")

    def step(self, batch: List[Sequence]):
        if self.prefill_flag:
            self.prefill_flag = False
            for sequence in batch:
                need_block_nums = sequence.seq_len + 1  # +1 for the initial block
                self.block_manager.allocate_gpu_blocks_for_all_layers(
                    sequence.seq_id, need_block_nums
                )
                sequence.block_num = need_block_nums
        # æ­¤å¤„åˆ†é…prefillï¼Œä¸€æ¬¡æ€§ä¸ºæ‰€æœ‰å±‚åˆ†é…æ‰€æœ‰promptçš„å—+1ï¼Œåªæœ‰ç¬¬ä¸€æ¬¡step
        for layer in range(self.model_config.num_attn_layers):
            # æ­¤å¤„åˆ†é…decodeé˜¶æ®µï¼Œæ¯ä¸€å±‚çš„æ–°å—ï¼Œç¬¬äºŒæ¬¡stepå¼€å§‹
            if not self.prefill_flag:
                for sequence in batch:
                    self.block_manager.allocate_gpu_blocks_for_layer(
                        sequence.seq_id, 1, layer
                    )
            print(f"layer {layer}")
            print(f"token nums of batch: {batch[0].seq_len}")
            print(f"len of gid_to_seq: {len(self.block_manager.gid_to_seq)}")
            self.layer_step(batch, layer)
            print(f"cpu block num:{self.block_manager.cpu_free_block_num()}")
            print(f"gpu block num:{self.block_manager.gpu_free_block_num()}")

    def layer_step(self, batch: List[Sequence], layer: int):
        # è¦ä¿è¯è¯¥å±‚çš„è®¡ç®—å¼€å§‹å‰ï¼Œæ‰€éœ€è¦çš„å—å·²ç»åˆ°ä½
        # éœ€è¦æœ‰ä¸€ä¸ªé˜Ÿåˆ—å°†CPUä¸­çš„å—æŒ‰ç…§ä½¿ç”¨å®ƒä»¬çš„é¡ºåºæ’å¥½é˜Ÿï¼Œ
        # ä»batchçš„å½“å‰å±‚å¼€å§‹çœ‹ï¼ŒæŠŠ
        # åˆ¤æ–­å½“å‰å±‚æ˜¯å¦é¢„å–å®Œæ¯•,ä»¥åŠå½“å‰å±‚æ˜¯å¦æ­£åœ¨é¢„å–ï¼Œå¦‚æœå®Œæ¯•ï¼Œåˆ™å¼€å§‹é¢„å–å°†æ¥æœ€è¿‘ä¸€å±‚éœ€è¦çš„å—ï¼Œå¦‚æœæ­£åœ¨é¢„å–ï¼Œåˆ™ç­‰å¾…é¢„å–å®Œæˆ
        # å¦‚æœè¯¥å±‚çš„kv cache not readyï¼Œè¯´æ˜é¢„å–çº¿ç¨‹ä¸€å®šåœ¨é¢„å–è¯¥å±‚çš„kv cache
        self.block_manager.wait_for_kv_cache_ready(batch, layer)
        print(f"ğŸ”µ Starting layer {layer} step with {len(batch)} sequences.")
        self.worker.execute_model(input_data=batch)  # Replace with actual input data

        # äº§ç”Ÿå½“å‰å±‚è®¡ç®—å®Œæ¯•çš„äº‹ä»¶

        # é€šçŸ¥ä¸Šä¸€å±‚çš„å¸è½½ä»»åŠ¡å¹¶åœ¨å…¶å®Œæˆå½“å‰ä¼ è¾“çš„åŸå­æ­¥éª¤åç»ˆæ­¢ä»»åŠ¡

        # å¼€å§‹æ‰§è¡Œå½“å‰å±‚çš„å¸è½½ä»»åŠ¡,å¹¶è‡ªåŠ¨ç»ˆæ­¢ä¸Šä¸€å±‚
        self.async_offloader.start_offload(layer)
        self.async_prefetcher.notify(layer)
        # é¢„å–åº”è¯¥æ”¾åœ¨è¿™é‡Œï¼Œå®ƒåº”è¯¥æ˜¯ä¸€ä¸ªå¸¸é©»çš„çº¿ç¨‹ï¼Œå•çº¯åœ°é€šè¿‡äº‹ä»¶æ¥åŒæ­¥
        # ä¹Ÿå°±æ˜¯è¯´ï¼Œé¢„å–çº¿ç¨‹ä¼šä¸€ç›´è¿è¡Œï¼Œä¸æ–­åœ°å°†åŠ ä¸‹æ¥æ‰€éœ€è¦çš„æ•°æ®ä»CPUä¼ è¾“åˆ°GPU
        # åœ¨æ¯å±‚çš„è®¡ç®—å¼€å§‹å‰ï¼Œé˜»å¡å½“å‰å±‚çš„é¢„å–ä»»åŠ¡ï¼Œ
        # å¦‚ä½•ä¿è¯ æœ‰æ–°çš„å—ä¾›åˆ†é…å‘¢ï¼Ÿç­”æ¡ˆæ˜¯è¦é¢„å–çš„æ¯”å¸è½½çš„è¦å°‘
        # è¿˜éœ€è¦ä¸€ä¸ªæŒ‡æ ‡æ¥è¯´æ˜å½“å‰è¶…é¢åˆ†é…äº†å¤šå°‘å—ï¼Œä¸‹ä¸€æ­¥æˆ–æœªæ¥å‡ æ­¥éœ€è¦å¤šå°‘å—ï¼Œé¢„å–å›æ¥çš„å—é™¤äº†ç¼“å†²åŒºä»¥å¤–
        # éœ€è¦æŠŠè¿™äº›å—ä¹Ÿç©ºå‡ºæ¥ï¼Œå¹¶ä¸”ä½œä¸ºé¢„å–å—æ•°çš„æŒ‡æ ‡
